{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "CIFAR10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakash3101/Deep-Learning/blob/master/ML/Deep_learning/pytorch_tutorials/cifar10/CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SFNZZIApVUJ"
      },
      "source": [
        "### **Transfer Learning using ResNet and other optimizations for CNNs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrxFB1KQpETi",
        "outputId": "b84c63b9-8ea0-4264-9435-4ff2b829820e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: agg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbG3-CWBqBnW"
      },
      "source": [
        "project_name = '05b-cifar10-resnet'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgOJ6h0vK9P"
      },
      "source": [
        "## **Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRbomAU0vJTt",
        "outputId": "53d2043b-2ffa-40e3-a572-728f4ddaafe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-sample/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "# Extract from archive\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "\n",
        "# Look into the data directory\n",
        "data_dir = './data/cifar10'\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./cifar10.tgz\n",
            "['test', 'train', 'labels.txt']\n",
            "['cat', 'horse', 'automobile', 'airplane', 'dog', 'ship', 'deer', 'bird', 'truck', 'frog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzWHYt5W7i2p",
        "outputId": "4f4b6af8-e79b-4108-bdd3-9dcdee24e9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(os.listdir(data_dir + '/train/horse'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8o6TgRs8DMj"
      },
      "source": [
        "There are a few important changes we'll make while creating the PyTorch datasets:\n",
        "\n",
        "1. **Use test set for validation:** Instead of setting aside a fraction (e.g. 10%) of the data from the training set for validation, we'll simply use the test set as our validation set **(Only when you have labels for your test set)**. This just gives a little more data to train with. In general, once you have picked the best model architecture & hyperparameters using a fixed validation set, it is a good idea to retrain the same model on the entire dataset just to give it a small final boost in performance\n",
        "\n",
        "2. **Channel-wise data normalization:** We will normalize the image tensors by subtracting the mean and dividing by the standard deviation across each channel. As a result, the mean of the data across each channel is 0, and standard deviation is 1. Normalizing the data prevents the values from any one channel from disproportionately affecting the losses and gradients while training, simply by having a higher or wider range of values than others.\n",
        "\n",
        "3. **Randomized data augmentations:** We will apply randomly chosen transformations while loading images from the training dataset. Specifically, we will pad each image by 4 pixels, and then take a random crop of size 32x32 pixels, and then flip the image horizontally with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPx71G8z-bqG"
      },
      "source": [
        "# Data transforms (normalization and data augmentation)\n",
        "\n",
        "# Mean and Standard Deviation for each color channel\n",
        "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "                         tt.RandomHorizontalFlip(),\n",
        "                         tt.ToTensor(),\n",
        "                         tt.Normalize(*stats, inplace=True)])\n",
        "\n",
        "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaLG8dp729g"
      },
      "source": [
        "# PyTorch datasets\n",
        "\n",
        "train_ds = ImageFolder(data_dir + '/train', train_tfms)\n",
        "val_ds = ImageFolder(data_dir + '/test', valid_tfms)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpWK-3nyLw8z"
      },
      "source": [
        "batch_size = 400"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYOmeleFzFWI"
      },
      "source": [
        "# PyTorch data loaders\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIgivqEEzZRO"
      },
      "source": [
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12,12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(images[:64], nrow=8).permute(1,2,0))\n",
        "        break"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-KPxzgzyes",
        "outputId": "ff607b1e-367f-4cf0-a758-8d032a5e2fe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "show_batch(train_dl)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1TZ3zCn0KMW"
      },
      "source": [
        "The colors seem out of place because of the normalization. Note that normalization is also applied during inference. If you look closely, you can see the cropping and reflection padding in some of the images. Horizontal flip is a bit difficult to detect from visual inspection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTK-ADy_0qKa"
      },
      "source": [
        "## **Using the GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER1M5PNDz2Hl"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensors to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-RK-u0N2GS6",
        "outputId": "36dc8752-e3da-4cce-f055-04f5206e9d6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z7vT2Kb2Kl7"
      },
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp3BjDsq2_Ko"
      },
      "source": [
        "## **Model with Residual Blocks and Batch Normalization**\n",
        "\n",
        "One of the key changes to our CNN model this time is the addition of the residual block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n",
        "\n",
        "        x     | - - - - - - - - -\n",
        "              |                  |\n",
        "        ( weight layer )         |\n",
        "              |                  |\n",
        "    F(x)      |     relu         |   x identity\n",
        "              |                  |\n",
        "        ( weight layer )         |\n",
        "              |                  |\n",
        "    F(x)+x   (+) - - - - - - - - -\n",
        "              |      relu\n",
        "              |\n",
        "              V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnNJT2I52Twf"
      },
      "source": [
        "class SimpleResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        return self.relu2(out) + x      # ReLU can be applied before or after adding the input"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zpDhK3l57Mq",
        "outputId": "ae0e12fc-d3e6-4f0a-8dd8-9f91b960bf3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "simple_resnet = to_device(SimpleResidualBlock(), device)\n",
        "\n",
        "for images, labels in train_dl:\n",
        "    out = simple_resnet(images)\n",
        "    print(out.shape)\n",
        "    break\n",
        "\n",
        "del simple_resnet, images, labels\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKZ6KgqxGC-P"
      },
      "source": [
        "## **Creating the model**\n",
        "\n",
        "We will be using the ResNet9 architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgPnoOov63zM"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                          # Generate Predictions\n",
        "        loss = F.cross_entropy(out, labels)         # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                          # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)         # Calculate loss\n",
        "        acc = accuracy(out, labels)                 # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()       # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()          # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v78kXKiwFHTh"
      },
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)        # batch_size x 64 x 32 x 32\n",
        "        self.conv2 = conv_block(64, 128, pool=True)     # batch_size x 128 x 16 x 16\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))   # batch_size x 128 x 16 x 16\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)    # batch_size x 256 x 8 x 8\n",
        "        self.conv4 = conv_block(256, 512, pool=True)    # batch_size x 512 x 4 x 4\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))   # batch_size x 512 x 4 x 4\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4),                # batch_size x 512 x 1 x 1\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvpmc1KEH4GN",
        "outputId": "63547e47-ca55-47e1-abe8-fe71e73ae284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model =  to_device(ResNet9(3, 10), device)\n",
        "model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet9(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res1): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res2): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "    (1): Flatten()\n",
              "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F5FfQIBIIf8"
      },
      "source": [
        "## **Training the Model**\n",
        "\n",
        "Before we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n",
        "\n",
        "- **Learning rate scheduling:** Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the \"*One Cycle Learning Rate Policy*\", which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% epochs, then gradually decreasing it to a very low value for the remaining epochs.\n",
        "\n",
        "- **Weight decay:** We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.\n",
        "\n",
        "- **Gradient clipping:** Apart from the layer weights and outputs, it's also helpful to limit the values 1:08:50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzdBYk7xH93T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XORltpjiKBy8"
      },
      "source": [
        "### Observation Table\n",
        "---\n",
        "Model 1:\n",
        "\n",
        "```\n",
        "```\n",
        "\n",
        "---\n",
        "| Model S.no | I (Lr, Epochs) | II (Lr, Epochs) | optimizer | val_acc | test_acc |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| 1 | (0.1, 20) | (0.01, 10) | SGD | 55.37 | 54.74 |\n",
        "| 1 | (0.2, 20) | (0.1, 10) | SGD | 49.40 | 48.86 |\n",
        "| 1 | (0.5, 20) | (0.1, 10) | SGD | 48.87 | 50.66 |\n",
        "| 1 | (0.1, 30) | (0.01, 10) | SGD | 55.66 | 55.31 |\n",
        "| 1 | (0.001, 20) | (0.0001, 10) | Adam | 53.04 | 53.34 |\n",
        "| 1 | (0.0001, 20) | (1e-5, 10) | Adam | 51.62 | 52.17 |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hfrORQ_EgVU"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}